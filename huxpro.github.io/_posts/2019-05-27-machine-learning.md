### 常见的机器学习方法
#### 逻辑回归
* 在线性回归的基础上，对输出y增加一个转换函数（sigmoid），使得结果集中在[0, 1], 而非所有实值上预测
* 损失函数：似然函数
* 防止过拟合：L1正则化（趋向于产生于少量的特征或者产生稀疏的权重）和L2正则化（选择更多的特征，这些特征接近于0；平滑的权值）
* 最大熵模型：多分类的逻辑回归

* 多分类（二分类模型）[参考](https://zhuanlan.zhihu.com/p/46599015)
 * 一对多（一个类别是正例，其他看做负例, K个分类器）
 * 一对一（两两类别训练一个分类器，投票所有分类器的结果， 分类器个数C(K, 2)））
 * softmax

#### 支持向量机
* 原理：找到一个超平面讲数据集进行正确的分类
* 优点
  * 在小数据集上往往得到比较好的结果（随机森林在大数据集上效果好）
  * 使用核函数避开了高维空间的复杂性
  * 泛化能力强
* 缺点
  * 时空开销大
  * 核函数难于选择，多靠经验

### Bagging（并行）
#### [随机森林](https://www.jianshu.com/p/57e862d695f2)
* 原理
  * 从原始数据集中有放回地采样出K个数据样本集，构建k个分类（回归）树
  * 对于每一颗分类（回归）树，从n个特征中随机选择m个特征，然后从这些特征找中找出最优的属性特征进行划分
  * 综合每颗树得到的分类（回归）结果，来预测最终的结果
* 分类：简单投票法；回归：简单平均法
* 优点
  * 计算开销小（每个只是选择m个数属性特征）
  * 增加了属性特征的随机扰动，随着基学习器的增多，随机森林会收敛于更低的泛化误差
  * 随机性的引入，使得随机森林不容易陷入过拟合
* 缺点
  * 噪声较大的时候容易过拟合

### Boosting（串行）
对训练样本分布调整，主要是增加误分类样本的权重，降低正确分类样本的权重
#### GBDT
* 核心：每一颗树学习的是之前所有树结论和的残差
* 残差就是全局最优的绝对方向
* 用到的树都是回归树
* 用于分类任务（CART）,回归（最小化均方误差）
* 总结：每一步的残差计算其实变相的增大了分错样本的权重，分对样本的权重为0
* 适用范围：回归（线性和非线性），二分类和多分类

#### 随机森林和GBDT对比
随机森林 | GBDT
-- | --
并行、弱依赖 | 串行、强依赖
分类树或者回归树 | 回归树
简单投票得到输出 | 加权累加
对异常值不敏感 | 对异常敏感
降低方差提高性能 | 降低偏差来提高性能